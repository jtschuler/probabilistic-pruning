{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages (and magic formulas), define model, and download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet34\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import copy\n",
    "import pruning_funcs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Load CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained EfficientNet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained EfficientNet\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/ Load Weights\n",
    "This cell is where the model is trained (lines 1-2), or loaded from the state dict (lines 4-7). \n",
    "The intention is to either do one or the other, not both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# for epoch in range(10):  # number of epochs\n",
    "#     model.train()\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "state_dict = torch.load(\"efficientnet_state\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(testing_model, dataloader, suppress_output=False):\n",
    "    testing_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = testing_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    if not suppress_output:\n",
    "        print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Laplace Pruning (Algorithm 3 in Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Laplace\n",
    "\n",
    "%autoreload now\n",
    "\n",
    "tests = np.arange(0.008, 0.081, 0.008)\n",
    "num_trials = 30\n",
    "\n",
    "print()\n",
    "print('Avg. accuracy at scale values for Normalized Laplace Distribution')\n",
    "\n",
    "print()\n",
    "print('Unpruned Model')\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader, suppress_output=True)\n",
    "percent_zero = pruning_funcs.percent_zero_weights(model)\n",
    "print(f'Accuracy: {accuracy:.2f}%\\tPercent Zero: {percent_zero:.2f}%')\n",
    "\n",
    "print()\n",
    "print(\"Pruned Models (Normalized Laplace)\")\n",
    "for prune_scale in tests:\n",
    "    accuracy = 0.\n",
    "    percent_zeros = 0.\n",
    "    for i in range(num_trials):\n",
    "        pruned_model = copy.deepcopy(model)\n",
    "        pruning_funcs.normalized_laplace_prune(pruned_model, device, scale=prune_scale)\n",
    "        accuracy += evaluate_model(pruned_model, test_loader, suppress_output=True)\n",
    "        percent_zeros += pruning_funcs.percent_zero_weights(pruned_model)\n",
    "    accuracy /= num_trials\n",
    "    percent_zeros /= num_trials\n",
    "    print(f'Scale: {prune_scale:.3f}\\tAvg. Accuracy: {accuracy:.2f}%\\tAvg. Percent Zero: {percent_zeros:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Percent by Layer pruning method (Algorithm 1 in Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned Models (Standard Percent Pruning By Layer)\n",
      "Theoretic percent pruned: 5%\tActual percent pruned: 5.00%\tAccuracy: 82.65%\n",
      "Theoretic percent pruned: 10%\tActual percent pruned: 10.00%\tAccuracy: 26.52%\n",
      "Theoretic percent pruned: 15%\tActual percent pruned: 15.00%\tAccuracy: 10.16%\n",
      "Theoretic percent pruned: 20%\tActual percent pruned: 20.00%\tAccuracy: 10.00%\n",
      "Theoretic percent pruned: 25%\tActual percent pruned: 25.00%\tAccuracy: 10.00%\n",
      "Theoretic percent pruned: 30%\tActual percent pruned: 30.00%\tAccuracy: 10.00%\n",
      "Theoretic percent pruned: 35%\tActual percent pruned: 35.00%\tAccuracy: 10.00%\n",
      "Theoretic percent pruned: 40%\tActual percent pruned: 40.00%\tAccuracy: 10.00%\n",
      "Theoretic percent pruned: 45%\tActual percent pruned: 45.00%\tAccuracy: 10.00%\n",
      "Theoretic percent pruned: 50%\tActual percent pruned: 50.00%\tAccuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Percent by Layer\n",
    "\n",
    "%autoreload now\n",
    "\n",
    "percents = np.arange(5,51,5)\n",
    "print()\n",
    "print(\"Pruned Models (Standard Percent Pruning By Layer)\")\n",
    "for percent in percents:\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruning_funcs.percent_prune_by_layer(pruned_model, device, percent=percent)\n",
    "    accuracy = evaluate_model(pruned_model, test_loader, suppress_output=True)\n",
    "    percent_zero = pruning_funcs.percent_zero_weights(pruned_model)\n",
    "    print(f'Theoretic percent pruned: {percent}%\\tActual percent pruned: {percent_zero:.2f}%\\tAccuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Prune Algorithm 4 in Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned Models (Standard Bottom Percent Pruning)\n",
      "Theoretic percent pruned: 5%\tActual percent pruned: 5.00%\tAccuracy: 92.98%\n",
      "Theoretic percent pruned: 10%\tActual percent pruned: 10.00%\tAccuracy: 92.89%\n",
      "Theoretic percent pruned: 15%\tActual percent pruned: 15.00%\tAccuracy: 92.89%\n",
      "Theoretic percent pruned: 20%\tActual percent pruned: 20.00%\tAccuracy: 92.80%\n",
      "Theoretic percent pruned: 25%\tActual percent pruned: 25.00%\tAccuracy: 92.80%\n",
      "Theoretic percent pruned: 30%\tActual percent pruned: 30.00%\tAccuracy: 92.22%\n",
      "Theoretic percent pruned: 35%\tActual percent pruned: 35.00%\tAccuracy: 92.04%\n",
      "Theoretic percent pruned: 40%\tActual percent pruned: 40.00%\tAccuracy: 90.31%\n",
      "Theoretic percent pruned: 45%\tActual percent pruned: 45.00%\tAccuracy: 88.48%\n",
      "Theoretic percent pruned: 50%\tActual percent pruned: 50.00%\tAccuracy: 79.47%\n"
     ]
    }
   ],
   "source": [
    "# Bottom Percent\n",
    "%autoreload now\n",
    "percents = np.arange(5,51,5)\n",
    "print()\n",
    "print(\"Pruned Models (Standard Bottom Percent Pruning)\")\n",
    "for percent in percents:\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruning_funcs.bottom_percent_prune(pruned_model, device, percent=percent)\n",
    "    accuracy = evaluate_model(pruned_model, test_loader, suppress_output=True)\n",
    "    percent_zero = pruning_funcs.percent_zero_weights(pruned_model)\n",
    "    print(f'Theoretic percent pruned: {percent}%\\tActual percent pruned: {percent_zero:.2f}%\\tAccuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stochastic Percent Pruning (Algorithm 2 in Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Pruning\n",
    "%autoreload now\n",
    "\n",
    "percent_prune = []\n",
    "percent_prune_with_bernoulli = []\n",
    "percents = np.arange(1,10,1)\n",
    "\n",
    "for percent in percents:\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruning_funcs.bottom_percent_prune(pruned_model, device, percent=percent)\n",
    "    percent_prune.append(evaluate_model(pruned_model, testloader, suppress_output=True))\n",
    "    \n",
    "    ppwb_runs = []\n",
    "    for i in range(30):\n",
    "        pruned_model_2 = copy.deepcopy(model)\n",
    "        pruning_funcs.percent_prune_with_bernoulli(pruned_model_2, device, percent=(2*percent), p_success=0.5)\n",
    "        ppwb_runs.append(evaluate_model(pruned_model_2, testloader, suppress_output=True))\n",
    "\n",
    "    percent_prune_with_bernoulli.append(np.mean(ppwb_runs))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Percent Pruning\")\n",
    "line1, = ax.plot(percents, percent_prune, color='blue')\n",
    "line2, = ax.plot(percents, percent_prune_with_bernoulli, color='orange')\n",
    "line1.set_label(\"Percent Pruning\")\n",
    "line2.set_label(\"Percent Pruning With Bernoulli (avg)\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Percent pruned (or expected pruned)\")\n",
    "ax.set_ylabel(\"Model accuracy\")\n",
    "ax.set_xticks(percents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
